{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "You'll need the latest version of the **azureml-ai-ml** package to run the code in this notebook. Run the cell below to verify that it is installed.\n",
    "\n",
    "> **Note**:\n",
    "> If the **azure-ai-ml** package is not installed, run `pip install azure-ai-ml` to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Workspace\n",
    "\n",
    "workspace_name = \"mlw-example\"\n",
    "\n",
    "ws_basic = Workspace(\n",
    "    name = workspace_name,  #your workspace name\n",
    "    location = \"eastus\",    #inwhich server location is used\n",
    "    display_name = \"Basic workspace-example\"    #name of the workspace displayed\n",
    "    description = \"This example shows how to create a basic workspace\"  #description\n",
    ")\n",
    "\n",
    "ml_client.workspace.begin_create(ws_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any file generated (and captured) from an experiment's run or job is an artifact. It may represent a model serialized as a Pickle file, the weights of a PyTorch or TensorFlow model, or even a text file containing the coefficients of a linear regression. Other artifacts can have nothing to do with the model itself, but they can contain configuration to run the model, pre-processing information, sample data, etc. As you can see, an artifact can come in any format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Azure Machine Learning, logging models has the following advantages:\n",
    "\n",
    "* You can deploy them on real-time or batch endpoints without providing an scoring script nor an environment.\n",
    "* When deployed, Model's deployments have a Swagger generated automatically and the Test feature can be used in Azure Machine Learning studio.\n",
    "* Models can be used as pipelines inputs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlfow\n",
    "import pickle\n",
    "\n",
    "filename = 'model.pkl'\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "mlflow.log_artifact(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to log model\n",
    "import mlflow\n",
    "mlflow.sklearn.log_model(sklearn_estimator, \"classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from acure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient(\n",
    "    credential = DefaultAzureCredential(),      # credentÅŸial to use for authenticaiton\n",
    "    subscription_id = subscription_id,          # Your subscription ID\n",
    "    resoruce_group_name = resoruce_group,       # The name of your resource group\n",
    "    workspace_name = workspace                  # The name of your worksapce\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the authentication, you need to call MLClient for the environment to connect to the workspace. You'll call MLClient anytime you want to create or update an asset or resource in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "\n",
    "# configure job\n",
    "job = command(\n",
    "    code = \"./src\",\n",
    "    command = \"python train.py\",\n",
    "    environment = \"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\",\n",
    "    compute = \"aml-cluster\",\n",
    "    experiment_name = \"train-model\"\n",
    ")\n",
    "\n",
    "# connect to workspace and submit job\n",
    "returned_job = ml_client.create_or_update(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the Azure CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the Azure CLI on your computer you can use a package manager.\n",
    "* Here are the instructions to install the Azure CLI, based on the platform you choose. https://learn.microsoft.com/en-us/cli/azure/install-azure-cli\n",
    "* You don't need to install the Azure CLI if you use the Azure Cloud Shell. Learn more about how to use the Azure Cloud Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Install the Azure Machine Learning extention (ml)\n",
    "> az extention add -n ml -y\n",
    "\n",
    "# create resource group\n",
    "> az group create --name \"rg-dp100-labs\" --location \"eastus\"\n",
    "\n",
    "# create workspace\n",
    "> az ml workspace create --name \"mlw-dp100-labs\" -g \"rg-dp100-labs\"\n",
    "\n",
    "# create a compute instance\n",
    "> az ml compute create --name \"ciXXXX\" --size STANDARD_DS11_V2 --type ComputeInstance -w mlw-dp100-labs -g rg-dp100-labs\n",
    "\n",
    "\n",
    "# create a compute cluster with 2 ways as followings\n",
    "> az ml compute create --name \"aml-cluster\" --size STANDARD_DS11_V2 --max-instances 2 --type AmlCompute -w mlw-dp100-labs -g rg-dp100-labs\n",
    "# or\n",
    "# Create the configuration in a YAML file\n",
    "# \".yml\" file \n",
    "\"\"\"\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/amlCompute.schema.json \n",
    "name: aml-cluster\n",
    "type: amlcompute\n",
    "size: STANDARD_DS3_v2\n",
    "min_instances: 0\n",
    "max_instances: 5\n",
    "\"\"\"\n",
    "> az ml compute create --file compute.yml --resource-group my-resource-group --workspace-name my-workspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For detailed information, one can visit here: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-datastore?view=azureml-api-2&tabs=sdk-identity-based-access%2Csdk-adls-identity-access%2Csdk-azfiles-accountkey%2Csdk-adlsgen1-identity-access%2Csdk-onelake-identity-access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Blob Storage container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by using account key to authentication\n",
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "from azure.ai.ml.entities import AccountKeyConfiguration\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client = MLClient.from_config()\n",
    "\n",
    "blob_datastore = AzureBlobDatastore(\n",
    "    name = \"blob_example\",\n",
    "    description = \"Datastore pointing to a blob container\",\n",
    "    account_name = \"mytestblobstore\",\n",
    "    container_name = \"data-container\",\n",
    "    credentials = AccountKeyCredentials(\n",
    "        account_key=\"XXXxxxXXXxXXXXxxXXX\"\n",
    "    ),\n",
    ")\n",
    "ml_client.create_or_update(blob_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by using a shared access signature (SAS) token to authentication\n",
    "from azure.ai.ml.entities import AzureBlobDatastore\n",
    "from azure.ai.ml.entities import SasTokenConfiguration\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "ml_client = MLClient.from_config()\n",
    "\n",
    "blob_datastore = AzureBlobDatastore(\n",
    "    name = \"blob_example\",\n",
    "    description = \"Datastore pointing to a blob container\",\n",
    "    account_name = \"mytestblobstore\",\n",
    "    container_name = \"data-container\",\n",
    "    credentials = SasTokenCredentials(\n",
    "        sas_token=\"?xx=XXXX-XX-XX&xx=xxxx&xxx=xxx&xx=xxxxxxxxxxx&xx=XXXX-XX-XXXXX:XX:XXX&xx=XXXX-XX-XXXXX:XX:XXX&xxx=xxxxx&xxx=XXxXXXxxxxxXXXXXXXxXxxxXXXXXxxXXXXXxXXXXxXXXxXXxXX\"\n",
    "    ),\n",
    ")\n",
    "ml_client.create_or_update(blob_datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a URI file data asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A URI file data asset points to a specific file. Azure Machine Learning will only store the path to the file, which means you can point to any type of file. When you use the data asset, you'll specify how you want to read the data, which will depend on the type of data you're connecting to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The supported paths you can use when creating a URI file data asset are:***\n",
    "\n",
    "* Local: ./<path>\n",
    "* Azure Blob Storage: wasbs://<account_name>.blob.core.windows.net/<container_name>/<folder>/<file>\n",
    "* Azure Data Lake Storage (Gen 2): abfss://<file_system>@<account_name>.dfs.core.windows.net/<folder>/<file>\n",
    "* Datastore: azureml://datastores/<datastore_name>/paths/<folder>/<file>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version=\"<version>\"\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you parse the URI file data asset as input in an Azure Machine Learning job, you'll first need to read the data before you can work with it.\n",
    "\n",
    "Imagine you create a Python script you want to run as a job, and you set the value of the input parameter input_data to be the URI file data asset (which points to a CSV file). You can read the data by including the following code in your Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "df = pd.read_csv(args.input_data)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your URI file data asset points to a different type of file, you'll need to use the appropriate Python code to read the data. For example, if instead of CSV files, you're working with JSON files, you'd use pd.read_json() instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a URI folder data asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A URI folder data asset points to a specific folder. It works similar to a URI file data asset and supports the same paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<supported-path>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you parse the URI folder data asset as input in an Azure Machine Learning job, you'll first need to read the data before you can work with it.\n",
    "\n",
    "Imagine you create a Python script you want to run as a job, and you set the value of the input parameter input_data to be the URI folder data asset (which points to multiple CSV files). You may want to read all CSV files in the folder and concatenate them, which you can do by including the following code in your Python script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_path = args.input_data\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the type of data you're working with, the code you use to read the files may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a MLTable data asset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MLTable data asset allows you to point to tabular data. When you create a MLTable data asset, you specify the schema definition to read the data. As the schema is already defined and stored with the data asset, you don't have to specify how to read the data when you use it.\n",
    "\n",
    "Therefore, you'll want to use a MLTable data asset when the schema of your data is complex or changes frequently. Instead of changing how to read the data in every script that uses the data, you only have to change it in the data asset itself.\n",
    "\n",
    "When you define the schema when creating a MLTable data asset, you can also choose to only specify a subset of the data.\n",
    "\n",
    "For certain features in Azure Machine Learning, like Automated Machine Learning, you'll need to use a MLTable data asset, as Azure Machine Learning needs to know how to read the data.\n",
    "\n",
    "To define the schema, it's recommended to include a MLTable file in the same folder as the data you want to read. The MLTable file includes the path pointing to the data you want to read, and how to read the data (above file is a yaml file):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type: mltable\n",
    "\n",
    "paths:\n",
    "  - pattern: ./*.txt\n",
    "transformations:\n",
    "  - read_delimited:\n",
    "      delimiter: ','\n",
    "      encoding: ascii\n",
    "      header: all_files_same_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = '<path-including-mltable-file>'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.MLTABLE,\n",
    "    description=\"<description>\",\n",
    "    name=\"<name>\",\n",
    "    version='<version>'\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import mltable\n",
    "import pandas\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input_data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "tbl = mltable.load(args.input_data)\n",
    "df = tbl.to_pandas_dataframe()\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach is to convert the tabular data to a Pandas data frame. However, you can also convert the data to a Spark data frame if that suits your workload better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
